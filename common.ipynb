{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1c27274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S字母:4072=qywioywioywioywiiE',\n",
       " 'S数字:9552=38211821182118208E',\n",
       " 'S数字:2761=11045104510451044E',\n",
       " 'S字母:9826=eoepuoepuoepuoeprE',\n",
       " 'S小写:1659=六六三六六六三六六六三六六六三六EP',\n",
       " 'S字母:1690=yuypyuypyuypyuypEP',\n",
       " 'S数字:7819=31279127912791276E',\n",
       " 'S小写:5754=二三〇一八三〇一八三〇一八三〇一六E',\n",
       " 'S字母:7683=epuetpuetpuetpuewE',\n",
       " 'S字母:3736=qrortrortrortrorrE',\n",
       " 'S大写:4882=壹玖伍贰玖玖伍贰玖玖伍贰玖玖伍贰捌E',\n",
       " 'S大写:6507=贰陆零叁零陆零叁零陆零叁零陆零贰捌E',\n",
       " 'S大写:2747=壹零玖捌玖零玖捌玖零玖捌玖零玖捌捌E',\n",
       " 'S字母:9709=eiieoiieoiieoiieyE',\n",
       " 'S数字:2022=8088808880888088EP',\n",
       " 'S大写:6868=贰柒肆柒肆柒肆柒肆柒肆柒肆柒肆柒贰E',\n",
       " 'S字母:3044=qwquuwquuwquuwquyE',\n",
       " 'S大写:3059=壹贰贰叁柒贰贰叁柒贰贰叁柒贰贰叁陆E',\n",
       " 'S数字:8316=33267326732673264E',\n",
       " 'S字母:6345=wteiwteiwteiwteipE',\n",
       " 'S大写:5408=贰壹陆叁肆壹陆叁肆壹陆叁肆壹陆叁贰E',\n",
       " 'S小写:2499=九九九六九九九六九九九六九九九六EP',\n",
       " 'S小写:5256=二一〇二六一〇二六一〇二六一〇二四E',\n",
       " 'S字母:7908=eqyetqyetqyetqyewE',\n",
       " 'S字母:9370=eurieurieurieuripE',\n",
       " 'S数字:2954=11817181718171816E',\n",
       " 'S字母:2502=qpppopppopppopppiE',\n",
       " 'S大写:5686=贰贰柒肆陆贰柒肆陆贰柒肆陆贰柒肆肆E',\n",
       " 'S数字:6815=27262726272627260E',\n",
       " 'S字母:8127=ewtqqwtqqwtqqwtpiE',\n",
       " 'S大写:4700=壹捌捌零壹捌捌零壹捌捌零壹捌捌零零E',\n",
       " 'S小写:8930=三五七二三五七二三五七二三五七二〇E',\n",
       " 'S小写:8921=三五六八七五六八七五六八七五六八四E',\n",
       " 'S数字:5157=20630063006300628E',\n",
       " 'S数字:3716=14865486548654864E',\n",
       " 'S数字:4017=16069606960696068E',\n",
       " 'S数字:8388=33555355535553552E',\n",
       " 'S字母:9948=eouotouotouotouowE',\n",
       " 'S小写:9258=三七〇三五七〇三五七〇三五七〇三二E',\n",
       " 'S数字:7244=28978897889788976E',\n",
       " 'S大写:4080=壹陆叁贰壹陆叁贰壹陆叁贰壹陆叁贰零E',\n",
       " 'S大写:2180=捌柒贰零捌柒贰零捌柒贰零捌柒贰零EP',\n",
       " 'S大写:6161=贰肆陆肆陆肆陆肆陆肆陆肆陆肆陆肆肆E',\n",
       " 'S大写:6739=贰陆玖伍捌陆玖伍捌陆玖伍捌陆玖伍陆E',\n",
       " 'S字母:9066=eywyuywyuywyuywyrE',\n",
       " 'S小写:3647=一四五八九四五八九四五八九四五八八E',\n",
       " 'S数字:1938=7752775277527752EP',\n",
       " 'S字母:5697=wwuopwuopwuopwuiiE',\n",
       " 'S大写:2855=壹壹肆贰壹壹肆贰壹壹肆贰壹壹肆贰零E',\n",
       " 'S字母:2435=ourpourpourpourpEP',\n",
       " 'S数字:2585=10341034103410340E',\n",
       " 'S数字:4648=18593859385938592E',\n",
       " 'S大写:4035=壹陆壹肆壹陆壹肆壹陆壹肆壹陆壹肆零E',\n",
       " 'S数字:3007=12029202920292028E',\n",
       " 'S字母:3959=qtieutieutieutieyE',\n",
       " 'S数字:2118=8472847284728472EP',\n",
       " 'S大写:7621=叁零肆捌柒零肆捌柒零肆捌柒零肆捌肆E',\n",
       " 'S大写:2758=壹壹零叁叁壹零叁叁壹零叁叁壹零叁贰E',\n",
       " 'S字母:3893=qttuettuettuettuwE',\n",
       " 'S字母:4279=quqquuqquuqquuqqyE',\n",
       " 'S小写:1149=四五九六四五九六四五九六四五九六EP',\n",
       " 'S字母:3158=qwyeewyeewyeewyewE',\n",
       " 'S小写:3810=一五二四一五二四一五二四一五二四〇E',\n",
       " 'S数字:9075=36303630363036300E']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.vocab = {\n",
    "            'mark': list('PSEU'),\n",
    "            'number': list('0123456789'),\n",
    "            'letter': list('pqwertyuio'),\n",
    "            'chinese_lower': list('〇一二三四五六七八九'),\n",
    "            'chinese_upper': list('零壹贰叁肆伍陆柒捌玖'),\n",
    "            'other': list('数字大写小母:=_'),\n",
    "        }\n",
    "\n",
    "        self.decoder = [j for i in self.vocab.values() for j in i]\n",
    "        self.encoder = {j: i for i, j in enumerate(self.decoder)}\n",
    "\n",
    "        self.label = {\n",
    "            'number': 0,\n",
    "            'letter': 1,\n",
    "            'chinese_lower': 2,\n",
    "            'chinese_upper': 3\n",
    "        }\n",
    "        self.prefix = ['数字', '字母', '小写', '大写']\n",
    "\n",
    "    def decode(self, x):\n",
    "        return ''.join([self.decoder[i] for i in x])\n",
    "\n",
    "    def get_data(self, prefix):\n",
    "        #生成问题和答案\n",
    "        question = random.randint(1000, 9999)\n",
    "        answer = int(str(question) * 4) * 4\n",
    "        #answer = question**8\n",
    "        \n",
    "        question = list(str(question))\n",
    "        answer = list(str(answer))\n",
    "\n",
    "        #随机label\n",
    "        label = random.choice(list(self.label.keys()))\n",
    "\n",
    "        #根据label替换答案成其他字符集\n",
    "        answer = [self.vocab[label][int(i)] for i in answer]\n",
    "\n",
    "        #label转数字\n",
    "        label = self.label[label]\n",
    "\n",
    "        #组合问题和答案\n",
    "        if prefix:\n",
    "            prefix = list(self.prefix[label])\n",
    "        else:\n",
    "            prefix = list('__')\n",
    "        token = prefix + [':'] + question + ['='] + answer\n",
    "\n",
    "        #编码\n",
    "        token = [self.encoder[i] for i in token]\n",
    "        token = [self.encoder['S']] + token + [self.encoder['E']]\n",
    "\n",
    "        return label, token\n",
    "\n",
    "    def get_batch_data(self, prefix):\n",
    "        data = [self.get_data(prefix=prefix) for _ in range(64)]\n",
    "\n",
    "        label = [i[0] for i in data]\n",
    "        token = [i[1] for i in data]\n",
    "\n",
    "        return label, *self.batch_pad(token=token)\n",
    "\n",
    "    def batch_pad(self, text=None, token=None):\n",
    "        if text:\n",
    "            #编码\n",
    "            token = [[self.encoder[j] for j in i] for i in text]\n",
    "\n",
    "        lens = max([len(i) for i in token])\n",
    "\n",
    "        input_ids = []\n",
    "        attention_mask = []\n",
    "        for i in token:\n",
    "            attention_mask.append([1] * len(i) + [0] * (lens - len(i)))\n",
    "            input_ids.append(i + [self.encoder['P']] * (lens - len(i)))\n",
    "\n",
    "        return input_ids, attention_mask\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# [tokenizer.decode(i) for i in tokenizer.get_batch_data(prefix=True)[1]][:10]\n",
    "[tokenizer.decode(i) for i in tokenizer.get_batch_data(prefix=True)[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e242caa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17208b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelGEN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        from transformers import GPT2Config, GPT2Model\n",
    "\n",
    "        self.config = GPT2Config(bos_token_id=tokenizer.encoder['S'],\n",
    "                                 eos_token_id=tokenizer.encoder['E'],\n",
    "                                 n_embd=64,\n",
    "                                 n_head=4,\n",
    "                                 n_layer=4,\n",
    "                                 n_positions=128,\n",
    "                                 vocab_size=len(tokenizer.decoder))\n",
    "\n",
    "        self.feature = GPT2Model(self.config)\n",
    "\n",
    "        self.fc_out = torch.nn.Linear(64, self.config.vocab_size, bias=False)\n",
    "\n",
    "        self.to(device)\n",
    "        self.train()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.feature(input_ids=input_ids,\n",
    "                           attention_mask=attention_mask).last_hidden_state\n",
    "\n",
    "        return self.fc_out(out)\n",
    "\n",
    "model_gen = ModelGEN().to(device)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a23b28dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelCLS(\n",
       "  (feature): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(53, 64, padding_idx=0)\n",
       "      (position_embeddings): Embedding(128, 64)\n",
       "      (token_type_embeddings): Embedding(2, 64)\n",
       "      (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-3): 4 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fc_out): Sequential(\n",
       "    (0): Dropout(p=0.1, inplace=False)\n",
       "    (1): Linear(in_features=64, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ModelCLS(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        from transformers import BertConfig, BertModel\n",
    "\n",
    "        self.config = BertConfig(hidden_size=64,\n",
    "                                 intermediate_size=64,\n",
    "                                 max_position_embeddings=128,\n",
    "                                 num_attention_heads=4,\n",
    "                                 num_hidden_layers=4,\n",
    "                                 vocab_size=len(tokenizer.decoder))\n",
    "\n",
    "        self.feature = BertModel(self.config)\n",
    "\n",
    "        self.fc_out = torch.nn.Sequential(torch.nn.Dropout(p=0.1),\n",
    "                                          torch.nn.Linear(64, 4))\n",
    "\n",
    "        self.to(device)\n",
    "        self.train()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.feature(input_ids=input_ids,\n",
    "                           attention_mask=attention_mask).pooler_output\n",
    "\n",
    "        return self.fc_out(out)\n",
    "\n",
    "model = ModelCLS().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6224a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelPPO(\n",
       "  (model_gen): ModelGEN(\n",
       "    (feature): GPT2Model(\n",
       "      (wte): Embedding(53, 64)\n",
       "      (wpe): Embedding(128, 64)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-3): 4 x GPT2Block(\n",
       "          (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (fc_out): Linear(in_features=64, out_features=53, bias=False)\n",
       "  )\n",
       "  (v_head): Sequential(\n",
       "    (0): Dropout(p=0.1, inplace=False)\n",
       "    (1): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ModelPPO(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, model_gen):\n",
    "        super().__init__()\n",
    "        self.model_gen = model_gen\n",
    "        self.v_head = torch.nn.Sequential(torch.nn.Dropout(0.1),\n",
    "                                          torch.nn.Linear(64, 1))\n",
    "\n",
    "        self.to(device)\n",
    "        self.train()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        last_hidden_state = self.model_gen.feature(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True).last_hidden_state\n",
    "\n",
    "        logits = self.model_gen.fc_out(last_hidden_state)\n",
    "        value = self.v_head(last_hidden_state).squeeze(-1)\n",
    "\n",
    "        return logits, value\n",
    "\n",
    "model = ModelPPO(model_gen).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a13f60dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "generater = None\n",
    "\n",
    "\n",
    "def generate(model_gen, input_ids):\n",
    "    global generater\n",
    "    if not generater:\n",
    "        #包装类,用于生成\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        generater = GPT2LMHeadModel(model_gen.config)\n",
    "        generater.transformer = model_gen.feature\n",
    "        generater.lm_head = model_gen.fc_out\n",
    "        generater.to(device)\n",
    "\n",
    "    return generater.generate(input_ids=input_ids,\n",
    "                              min_length=-1,\n",
    "                              top_k=0.0,\n",
    "                              top_p=1.0,\n",
    "                              do_sample=True,\n",
    "                              pad_token_id=tokenizer.encoder['P'],\n",
    "                              max_new_tokens=25,\n",
    "                              eos_token_id=tokenizer.encoder['E'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
